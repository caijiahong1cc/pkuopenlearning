# 统计学习及监督学习概论

## 一、概论（特点等）

略过

## 二、统计学习分类

### 1.2.1基本分类

统计学习一般包括监督学习、无监督学习、强化学习、（半监督学习、主动学习）

#### 监督学习

监督学习指从标注数据中学习预测模型的机器学习问题，其本质是学习输入到输出的映射的统计规律。

输入空间、特征空间和输出空间：

（1）输入、输出的全体可能取值（有限元素集合或欧氏空间）的集合——输入、输出空间

（2）输入是实例、通常由特征向量表示，所有特征向量存在的空间称为特征空间

根据输入变量和输出变量的不同类型划分预测任务：

（1）输入与输出均为连续变量的预测问题称为**回归问题**；

（2）输出变量为有限个离散变量的预测问题称为**分类问题**；

（3）输入与输出变量均为变量序列的预测问题称为**标注问题**

![image-20200921185206147](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921185206147.png)

**假设空间**：映射集合

#### 无监督学习

无监督学习指从无标注数据中学习预测模型的机器学习问题，其本质是学习数据中的统计规律或内在结构。

无监督学习旨在从假设空间中选出在给定评价标准下的最优模型，模型可以实现对数据的聚类、降维或是概率估计。

![image-20200921190322838](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921190322838.png)

#### 强化学习

强化学习指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题，其本质是学习最优的序贯决策。

智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化。强化学习过程中，系统不断地试错，以达到学习最优策略地目的。

**变量解释：**

S是有限状态集合，A是有限动作集合，P是状态转移概率函数，r是奖励函数，γ衰减系数

策略Π定义为给定状态下动作的函数f（s），给定策略即给定智能系统与环境的互动行为

价值函数或状态价值函数定义为策略Π从某一个状态s开始的长期累积奖励的数学期望

动作价值函数定义为策略Π的从某一个状态s和动作a开始的长期累积奖励的数学期望

强化学习的目标就是在所有可能的策略中选出价值函数最大的策略Π，并不断优化已有策略

强化学习方法具体分为有模型和无模型的，体现为是否直接学习马尔可夫决策过程的模型

#### 半监督学习与主动学习（“更接近监督学习”）

半监督学习：数据构成由一部分已标注和一部分未标注组成

主动学习：由机器主动给出实例让教师进行标注

### 1.2.2 按模型分类

#### 概率模型与非概率模型

概率模型讲概率最大，非概率模型讲函数

#### 线性模型与非线性模型

即y=f（x）是否为线性函数

常见的线性模型：感知机、线性支持向量机、k近邻、k均值、潜在语义分析

常见的非线性模型：核函数支持向量机、AdaBoost、神经网络

#### 参数化模型与非参数化模型

即模型参数的维度是否固定

### 1.2.3按算法分类

### 1.2.4按技巧分类

#### 贝叶斯学习

根据贝叶斯定理计算后验概率

![image-20200921214126159](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921214126159.png)

预测时，计算数据对后验概率分布的期望值

![image-20200921214158828](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921214158828.png)

贝叶斯学习与极大似然估计的联系：

若认为先验分布是均匀分布，也即P（θ）固定，P（D）为已知值，则贝叶斯估计的后验概率最大转化为极大似然函数，从而将两者联系起来，即从贝叶斯估计能够得到最大似然估计

#### 核方法

使用和函数表示和学习非线性模型的一种机器学习方法，可以用于监督学习和无监督学习

理解核函数：从输入空间到特征空间的映射

![image-20200921215735537](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921215735537.png)

## 三、统计学习方法三要素

### 1.3.1 模型

在监督学习中，模型就是所要学习的条件概率分布或决策函数

（1）假设空间：决策函数的集合

（2）参数空间：

### 1.3.2 策略

**损失函数**：

模型的预测值与真实值可能不绝对一样，以此来度量预测错误的程度，记作L（Y，f（X））

常用的损失函数：

![image-20200921221205625](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921221205625.png)

![image-20200921221218835](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921221218835.png)

![image-20200921221240153](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921221240153.png)

损失函数的期望。称为风险函数或期望损失

![image-20200921221348568](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921221348568.png)

给定测试数据集，f（X）关于训练数据集的平均损失称为经验风险或经验损失

![image-20200921221535439](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921221535439.png)

当N趋向于无穷，经验分享趋向于风险函数，即上述两式趋向于相等

因此可以用经验风险估计期望风险，但训练样本数目有限，不能达到无穷的要求，由此引出两种基本策略

**经验风险最小化（ERM）：**

需求：样本容量足够大，样本容量小时出现过拟合

典型例子：极大似然估计

![image-20200921222137111](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921222137111.png)

**结构风险最小化（SRM）：**

背景：防止过拟合

![image-20200921222239346](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921222239346.png)

J（f）表示模型复杂度

典型例子：贝叶斯估计中的最大后验概率估计

### 1.3.3 算法

## 四、模型评估与模型选择

### 1.4.1 训练误差与测试误差

将学习方法对未知数据的预测能力称为泛化能力

### 1.4.2 过拟合与模型选择

如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高，称为过拟合

对已知数据预测的很好，但对未知数据预测得很差

 ![image-20200921232253427](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921232253427.png)

## 五、正则化与交叉验证（模型的选择方法）

### 1.5.1 正则化

正则化是结构风险最小化策略的实现，在经验分享上加一个正则化项或罚项

正则化项一般是模型复杂度的单调递增函数

![image-20200921232611145](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200921232611145.png)

**补充知识：**

L0范数：指向量中非0的元素的个数

L1范数：指向量中各个元素绝对值之和

L2范数：向量各元素的平方和然后求平方根



正则化项可以是参数向量的L2范数，也可以是参数向量的L1范数

正则化的作用是选择经验风险与模型复杂度同时小的模型

正则化符合奥卡姆剃刀原理：如无必要，勿增实体。在应用于模型选择中时，可以理解为：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的才是最好的模型，也是应该选择的模型。

### 1.5.2 交叉验证

若数据充足，最理想的方法是：

将数据集随机的切分成三个部分，分别为训练集，测试集和验证集。训练集用于训练模型，验证集用于模型选择，测试集用于模型评估。

但大多数情况下数据不足，引入交叉验证，重复使用数据

（1）简单交叉验证

测试集&&训练集，用训练集在各种条件下训练模型，得到不同的模型，在测试集上评价各模型的测试误差

（2）S折交叉验证

首先随机地将已给数据切分为S个互不相交、大小相同的子集 ;然后利用 S-l 个子集的数据训练模型，利用余下的子集测试模型 ，将这一过程对可能的S种选择重复进行;最后选出S次评测中平均测试误差最小的模型。

（3）留一交叉验证

S=N，N是给定数据集的容量

## 六、泛化能力

### 1.6.1 泛化误差

学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称为泛化误差上界

泛化误差上界是样本容量的函数，样本容量增加时，泛化上界趋向于0

泛化误差上界是假设空间容量的函数，空间容量越大，模型越难学，泛化误差上界就越大

数学表示：

![image-20200922103907470](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200922103907470.png)

## 七、生成模型与判别模型

监督学习方法可分为生成方法和判别方法，所学到的模型分为生成模型和判别模型

#### 生成方法：

（1）通过数据学习联合概率，然后求出条件概率分布

![image-20200922110019065](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200922110019065.png)

模型给定了输入x输出y的生成关系

（2）可以还原出**联合概率分布**

（3）收敛速度快, 当样本容量增加时, 学到的模型可以更快收敛到真实模型

（4）存在隐变量可用

#### 判别方法：

（1）判别方法直接学习决策函数或条件概率分布直观的观察输入x应该输出什么Y

（2）直接面对预测, 往往学习准确率更高

（3）可以对数据进行各种程度的抽象, 定义特征并使用特征, 可以简化学习问题

## 八、监督学习应用



