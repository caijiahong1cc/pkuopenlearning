# 第五章-决策树

它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布

学习时，利用训练数据，根据损失函数最小化的而原则建立决策树模型，预测时，对新的数据，利用决策树模型进行分类

决策树学习包括：特征提取、决策树的生成和决策树的修剪

## 5.1 决策树模型与学习

### 5.1.1 决策树模型

#### 定义5.1 决策树

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。节点有两种类型：内部节点和叶结点

内部结点表示一个特征或属性，叶节点表示一个类

分类方法：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点，如此递归的测试和分配，直到将实例分到叶节点的类中

### 5.1.2 决策树与if-then规则

由决策树的根节点到叶节点的每一条路径构成一条规则，路径上内部结点的特征对应着规则的条件，而叶节点的类对应着规则的结论

决策树的路径或其对应的if-then规则集合具有一个重要性质：互斥且完备

### 5.1.3 决策树与条件概率分布

决策树还表示给定特征条件下类的条件概率分布

将特征空间划分为互不相交额单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布

### 5.1.4 决策树学习

目标是根据给定的训练数据构建一个决策树模型，使它能够对给定的实例进行正确的分类

构建决策树的过程：

（1）构建根节点，并选择一个最优特征对训练数据进行分类，从而将数据集分割成子集，将子集分到相应的叶节点中去

（2）如此进行递归，直到所有的数据训练子集都被基本正确的分类

（3）这样得到的决策树有可能发生过拟合现象，因此需要进行从下到上的剪枝操作，去掉过于细分的叶节点

决策树学习常用 的算法有ID3、C4.5与CART

## 5.2 特征选择

### 5.2.1 特征选择问题

引入信息增益表示特征的分类能力

### 5.2.2 信息增益

熵：表示随机变量不确定性的度量

![image-20200927110809112](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927110809112.png)

熵越大，表示随机变量的不确定性就越大

当随即变量只取两个值时，例如0、1，熵为

![image-20200927110917259](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927110917259.png)

分布为伯努利分布时熵与概率的关系

![image-20200927110944520](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927110944520.png)

条件熵：

![image-20200927111656149](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927111656149.png)

当熵中的概率由数据估计得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵



信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度

#### 定义5.2 信息增益

![image-20200927112205736](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927112205736.png)

一般的，决策树学习中的信息增益等价于训练集中类与特征的互信息

特征选择：信息增益依赖于特征，不同的特征往往具有不同的信息增益，**信息增益大的特征具有更强的分类能力**

#### 算法 5.1 信息增益的算法

![image-20200927114048958](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927114048958.png)

### 5.2.3 信息增益比

如果单纯以信息增益作为划分训练数据集的特征，存在偏向于取值较多的特征的问题，因此引入信息增益比对其进行校正

#### 定义5.3 信息增益比

![image-20200927114457199](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927114457199.png)

## 5.3 决策树的生成

### 5.3.1 ID3算法

从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由特征的不同取值建立子节点，并对子节点递归的调用以上方法，从而构建决策树

#### 算法5.2 ID3算法

![image-20200927144307216](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927144307216.png)

![image-20200927144315587](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927144315587.png)

### 5.3.2 C4.5的生成算法

C4.5算法与ID3算法相似，但提出了改进：在特征选择时，使用增益比代替信息增益

#### 算法5.3（C4.5的生成算法）

![image-20200927144537909](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927144537909.png)

## 5.4 决策树的剪枝

目的是解决过拟合的现象，简化决策树的复杂度

决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现

定义决策树学习的损失函数为：

![image-20200927145106917](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927145106917.png)

其中经验熵为：

![image-20200927145152820](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927145152820.png)

将经验熵代入损失函数右边第一项，从而消掉Nt，得到

![image-20200927145519555](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927145519555.png)

从而损失函数简化为

![image-20200927145548812](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927145548812.png)

这条式子中，C（T）表示模型对训练数据的预测误差，α|T|是模型复杂度的函数，|T|表示模型复杂度，α控制模型复杂度的影响

α较大时，促使选择较简单的模型，使总体较小

α较小时，促使选择较复杂的模型，使总体较大

α==0时，表明只考虑模型与训练数据的拟合度，不考虑模型的复杂度的影响

**剪枝，即是当α确定时，选择损失函数最小的模型**

子树的复杂度与拟合度成正比，但也与模型复杂度成正比

#### 算法5.4 剪枝算法

![image-20200927150343641](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927150343641.png)

![image-20200927150353333](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927150353333.png)

算法提示：可以由动态规划的思想实现

## 5.5 CART算法

全称为分类与回归树模型，同样由特征选择、树的生成以及剪枝组成，既可以用于分类也可以用于回归

**CART假设决策树是二叉树**

### 5.5.1 CART生成

递归地构建二叉树，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树

CART假设**决策树是二叉树**，内部节点特征的取值均为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元。

CART算法由两步组成：

（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量最大；

（2）决策树剪枝：用验证集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝标准

介绍生成算法前，先引入基尼指数

#### 定义5.4 基尼指数

![image-20200927153006114](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927153006114.png)

基尼指数Gini（D）表示集合D的不确定性，基尼指数越大，样本集合的不确定性也就越大

除此之外，基尼指数随概率变化的曲线与二分之一熵的曲线很接近，**因此都可以近似的代表分类误差率**

#### 算法5.6 CART生成算法

输入：训练数据集![[公式]](https://www.zhihu.com/equation?tex=D)，停止计算条件；

输出：CART决策树

根据训练集，从根节点开始，递归地对每个节点进行如下操作，构建二叉决策树：

（1）设节点的训练集为![[公式]](https://www.zhihu.com/equation?tex=D)，利用公式![[公式]](https://www.zhihu.com/equation?tex=%282%29)计算现有特征对该数据集的基尼指数。此时，对于每一个特征![[公式]](https://www.zhihu.com/equation?tex=A)，对其可能的每一个值![[公式]](https://www.zhihu.com/equation?tex=a)，根据样本点对![[公式]](https://www.zhihu.com/equation?tex=A%3Da)的测试为“是”或“否”将![[公式]](https://www.zhihu.com/equation?tex=D)分割成![[公式]](https://www.zhihu.com/equation?tex=D_1%2CD_2)两个部分，利用公式![[公式]](https://www.zhihu.com/equation?tex=%283%29)计算![[公式]](https://www.zhihu.com/equation?tex=A%3Da)时的基尼指数；

（2）在所有可能的特征![[公式]](https://www.zhihu.com/equation?tex=A)以及它们所有可能的切分点![[公式]](https://www.zhihu.com/equation?tex=a)中，选择**基尼指数最小的特征**作为划分标准将原有数据集划分为两个部分，并分配到两个子节点中去；

（3）对两个子节点递归的调用(1),(2)，直到满足停止条件；

（4）生成CART决策树

其中，算法停止计算的条件是：节点中的样本点个数小于预定阈值，或样本集的基尼指数小于预定阈值（**也就是说此时样本基本属于同一类**），或者没有更多特征。

#### 例5.4 根据CART算法生成决策树

### 5.5.2 CART剪枝

#### 1、剪枝，形成一个子序列

在剪枝过程中，计算子树的损失函数：

![[公式]](https://www.zhihu.com/equation?tex=C_%7B%5Calpha%7D%28T%29%3DC%28T%29%2B%5Calpha%7CT%7C%5Ctag%7B4%7D+)

其中，![[公式]](https://www.zhihu.com/equation?tex=T)为任意子树，![[公式]](https://www.zhihu.com/equation?tex=C%28T%29)为对训练集的预测误差，![[公式]](https://www.zhihu.com/equation?tex=%7CT%7C)为子树的叶节点个数，![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%5Cgeq0)为参数。需要指出的是不同与之前ID3和C4.5中剪枝算法的![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)，前者是人为给定的，而此处则是通过计算得到。

首先定义一个比较量：单节点的损失函数：

![[公式]](https://www.zhihu.com/equation?tex=C_%7B%5Calpha%7D%28t%29%3DC%28t%29%2B%5Calpha%5Ccdot1%5Ctag%7B6%7D+)

然后，考虑α的值变化对剪枝选择的影响

①当![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%3D0)或者极小的时候，有不等式

![[公式]](https://www.zhihu.com/equation?tex=C_%7B%5Calpha%7D%28T_t%29%3C+C_%7B%5Calpha%7D%28t%29%5Ctag%7B7%7D+)

不等式成立的原因是因为，当![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%3D0)或者极小的时候，起决定作用的就是预测误差![[公式]](https://www.zhihu.com/equation?tex=C%28t%29%2CC%28T_t%29)，而模型越复杂其训练误差总是越小的，因此不等式成立。

②当![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)增大时，在某一![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)有

![[公式]](https://www.zhihu.com/equation?tex=C_%7B%5Calpha%7D%28T_t%29%3DC_%7B%5Calpha%7D%28t%29%5Ctag%7B8%7D+)

等式成立的原因是因为，当![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)慢慢增大时，就不能忽略模型复杂度所带来的影响（也就是式子![[公式]](https://www.zhihu.com/equation?tex=%284%29)第二项。但由于相同取值的![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)对于式子![[公式]](https://www.zhihu.com/equation?tex=%285%29%286%29)所对应模型的惩罚力度不同（剪枝前的惩罚力度更大），因此尽管式子![[公式]](https://www.zhihu.com/equation?tex=%285%29%286%29)所对应的模型复杂度均在减小（误差变大），但是![[公式]](https://www.zhihu.com/equation?tex=%285%29)较小得更快（误差变大得更快），所以总有个时候等式会成立。

③当![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)再增大时，不等式![[公式]](https://www.zhihu.com/equation?tex=%288%29)反向。因此，当![[公式]](https://www.zhihu.com/equation?tex=C_%7B%5Calpha%7D%28T_t%29%3DC_%7B%5Calpha%7D%28t%29)时，有![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%3D%5Cfrac%7BC%28t%29-C%28T_t%29%7D%7B%7CT_t%7C-1%7D)，此时的子树![[公式]](https://www.zhihu.com/equation?tex=T_t)和单节点 树![[公式]](https://www.zhihu.com/equation?tex=t)有相同的损失函数值，但![[公式]](https://www.zhihu.com/equation?tex=t)的节点少模型更简单，因此![[公式]](https://www.zhihu.com/equation?tex=t)比![[公式]](https://www.zhihu.com/equation?tex=T_t)更可取，即对![[公式]](https://www.zhihu.com/equation?tex=T_t)进行剪枝。（注：此时的![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)是通过![[公式]](https://www.zhihu.com/equation?tex=C_%7B%5Calpha%7D%28T_t%29%3DC_%7B%5Calpha%7D%28t%29)计算得到)

为此，对决策树![[公式]](https://www.zhihu.com/equation?tex=T_0)中每一个内部节点![[公式]](https://www.zhihu.com/equation?tex=t)来说，都可以计算

![[公式]](https://www.zhihu.com/equation?tex=g%28t%29%3D%5Cfrac%7BC%28t%29-C%28T_t%29%7D%7B%7CT_t%7C-1%7D%5Ctag%7B9%7D+)

它表示剪枝后整体损失函数减少的程度。因为每个![[公式]](https://www.zhihu.com/equation?tex=g%28t%29)背后都对应着一个决策树模型，而不同的![[公式]](https://www.zhihu.com/equation?tex=g%28t%29)则表示损失函数变化的不同程度。接着，在树![[公式]](https://www.zhihu.com/equation?tex=T_0)中减去![[公式]](https://www.zhihu.com/equation?tex=g%28t%29)最小的子树![[公式]](https://www.zhihu.com/equation?tex=T_t)，将得到的子树作为![[公式]](https://www.zhihu.com/equation?tex=T_1)。如此剪枝下去，直到得到根节点。

这里还对为什么选择g（t）做补充说明：

对于树![[公式]](https://www.zhihu.com/equation?tex=T)来说，其内部可能的节点![[公式]](https://www.zhihu.com/equation?tex=t)有![[公式]](https://www.zhihu.com/equation?tex=t_0%2Ct_1%2Ct_2%2Ct_3)；![[公式]](https://www.zhihu.com/equation?tex=t_i)表示其中任意一个。因此我们便可以计算得到![[公式]](https://www.zhihu.com/equation?tex=g%28t_0%29%2Cg%28t_1%29%2Cg%28t_2%29%2Cg%28t_3%29)，也即对应的![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_0%2C%5Calpha_1%2C%5Calpha_2%2C%5Calpha_3)。从上面的第③种情况我们可以知道，![[公式]](https://www.zhihu.com/equation?tex=g%28t%29)是根据公式![[公式]](https://www.zhihu.com/equation?tex=%289%29)所计算得到，因此这四种情况下![[公式]](https://www.zhihu.com/equation?tex=t_i)比![[公式]](https://www.zhihu.com/equation?tex=T_%7Bt_i%7D)更可取，都满足剪枝。但是由于以![[公式]](https://www.zhihu.com/equation?tex=t_i)为根节点的子树对应的复杂度各不相同，也就意味着![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_i%5Cneq%5Calpha_j%2C%28i%2Cj%3D0%2C1%2C2%2C3%3Bi%5Cneq+j%29),即![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_i%2C%5Calpha_j)存在着大小关系。又因为我们知道：**当![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)大的时候，最优子树![[公式]](https://www.zhihu.com/equation?tex=T_%7B%5Calpha%7D)偏小；当![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)小的时候，最优子树![[公式]](https://www.zhihu.com/equation?tex=T_%7B%5Calpha%7D)偏大**；且子树偏大意味着拟合程度更好。因此，在都满足剪枝的条件下，选择拟合程度更高的子树当然是最好的选择。所有选择减去其中![[公式]](https://www.zhihu.com/equation?tex=g%28t%29)最小的子树。

#### 2、交叉验证选择最优子树

利用独立的验证数据集，测试子树序列T0，T1.......Tn中各棵子树中的平方误差或基尼指数，值最小的即可确定为最优决策树

#### 算法5.7 CART剪枝算法

![image-20200927163706146](C:\Users\赖凯庭\AppData\Roaming\Typora\typora-user-images\image-20200927163706146.png)

## 本章概要

1、分类决策树可以转换为一个if-then 的集合，也可以看作是定义在特征空间划分上的类的条件概率的分布

2、决策树的构建目标是一个与训练数据拟合程度好，但同时尽可能复杂度小的决策树

3、决策树学习算法包括三个步骤：特征选择-->树的生成-->树的剪枝

常用的算法有ID3（信息增益选大）、C4.5（信息增益比选大）、CART（基尼系数选小）

4、由于存在过拟合问题，需要对决策树进行剪枝

## 参考文章：

知乎专栏（对CART算法细致清晰的讲解）：https://zhuanlan.zhihu.com/p/145215188